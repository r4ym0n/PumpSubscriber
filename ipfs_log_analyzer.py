#!/usr/bin/env python3
"""
IPFS Gateway Performance Log Analyzer

åˆ†ærs_subscriberäº§ç”Ÿçš„IPFSç½‘å…³æ€§èƒ½æ—¥å¿—ï¼Œä¸»è¦å…³æ³¨ipfs_pull_doneäº‹ä»¶ã€‚
æä¾›ç½‘å…³æ€§èƒ½å¯¹æ¯”ã€æ—¶åºåˆ†æã€win rateè®¡ç®—ç­‰åŠŸèƒ½ã€‚
"""

import json
import re
import sys
from datetime import datetime
from collections import defaultdict, Counter
from typing import Dict, List, Tuple, Optional
import statistics
import argparse


class IPFSLogEntry:
    """IPFSæ—¥å¿—æ¡ç›®æ•°æ®ç»“æ„"""
    
    def __init__(self, timestamp: str, data: dict):
        self.timestamp = timestamp
        self.data = data
        self.bytes = data.get('bytes', 0)
        self.elapsed_ms = data.get('elapsed_ms', 0)
        self.gateway = data.get('gateway', '')
        self.speed_kbps = data.get('speed_kbps', 0)
        self.mint = data.get('mint', '')
        self.subject = data.get('subject', '')
        
    @property
    def datetime(self) -> datetime:
        """è§£ææ—¶é—´æˆ³ä¸ºdatetimeå¯¹è±¡"""
        try:
            return datetime.strptime(self.timestamp, "%Y-%m-%d %H:%M:%S.%f")
        except ValueError:
            try:
                return datetime.strptime(self.timestamp, "%Y-%m-%d %H:%M:%S")
            except ValueError:
                return datetime.now()


class IPFSLogAnalyzer:
    """IPFSæ—¥å¿—åˆ†æå™¨"""
    
    def __init__(self):
        self.entries: List[IPFSLogEntry] = []
        self.gateway_stats: Dict[str, dict] = defaultdict(lambda: {
            'total_requests': 0,
            'total_bytes': 0,
            'total_elapsed_ms': 0,
            'elapsed_times': [],
            'speeds': [],
            'file_sizes': []
        })
        
    def parse_log_line(self, line: str) -> Optional[IPFSLogEntry]:
        """è§£æå•è¡Œæ—¥å¿—"""
        line = line.strip()
        if not line:
            return None
            
        # å¤„ç†Dockeræ—¥å¿—æ ¼å¼ï¼Œç§»é™¤å®¹å™¨å‰ç¼€
        # æ ¼å¼: rs-subscriber  | [2025-09-19 01:05:52.073] {"bytes":115303,"elapsed_ms":1154,...}
        # æˆ–è€…: [2025-09-19 01:05:52.073] {"bytes":115303,"elapsed_ms":1154,...}
        
        # ç§»é™¤Dockerå®¹å™¨å‰ç¼€ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        docker_prefix_pattern = r'^[^|]*\|\s*'
        line = re.sub(docker_prefix_pattern, '', line)
        
        # æå–æ—¶é—´æˆ³å’ŒJSONæ•°æ®
        match = re.match(r'\[([^\]]+)\]\s*(.+)', line)
        if not match:
            return None
            
        timestamp_str = match.group(1)
        json_str = match.group(2)
        
        try:
            data = json.loads(json_str)
            # åªå¤„ç†ipfs_pull_doneäº‹ä»¶
            if data.get('event') == 'ipfs_pull_done':
                return IPFSLogEntry(timestamp_str, data)
        except json.JSONDecodeError:
            pass
            
        return None
    
    def load_log_file(self, file_path: str):
        """åŠ è½½æ—¥å¿—æ–‡ä»¶"""
        print(f"æ­£åœ¨åŠ è½½æ—¥å¿—æ–‡ä»¶: {file_path}")
        
        with open(file_path, 'r', encoding='utf-8') as f:
            for line_num, line in enumerate(f, 1):
                entry = self.parse_log_line(line)
                if entry:
                    self.entries.append(entry)
                    
        print(f"æˆåŠŸè§£æ {len(self.entries)} æ¡ ipfs_pull_done è®°å½•")
        
    def load_log_from_stdin(self):
        """ä»æ ‡å‡†è¾“å…¥åŠ è½½æ—¥å¿—"""
        print("æ­£åœ¨ä»æ ‡å‡†è¾“å…¥è¯»å–æ—¥å¿—...")
        
        for line_num, line in enumerate(sys.stdin, 1):
            entry = self.parse_log_line(line)
            if entry:
                self.entries.append(entry)
                
        print(f"æˆåŠŸè§£æ {len(self.entries)} æ¡ ipfs_pull_done è®°å½•")
    
    def calculate_stats(self):
        """è®¡ç®—ç»Ÿè®¡ä¿¡æ¯"""
        print("æ­£åœ¨è®¡ç®—ç»Ÿè®¡ä¿¡æ¯...")
        
        for entry in self.entries:
            gateway = entry.gateway
            stats = self.gateway_stats[gateway]
            
            stats['total_requests'] += 1
            stats['total_bytes'] += entry.bytes
            stats['total_elapsed_ms'] += entry.elapsed_ms
            stats['elapsed_times'].append(entry.elapsed_ms)
            stats['speeds'].append(entry.speed_kbps)
            stats['file_sizes'].append(entry.bytes)
    
    def calculate_latency_distribution(self, elapsed_times: List[int]) -> Dict[str, dict]:
        """è®¡ç®—è€—æ—¶åˆ†å¸ƒç»Ÿè®¡"""
        # å®šä¹‰è€—æ—¶åŒºé—´ (æ¯«ç§’)
        buckets = [
            ('0-100ms', 0, 100),
            ('100-500ms', 100, 500),
            ('500-2000ms', 500, 2000),
            ('2000ms+', 2000, float('inf'))
        ]
        
        distribution = {}
        total_count = len(elapsed_times)
        
        for bucket_name, min_val, max_val in buckets:
            if max_val == float('inf'):
                count = sum(1 for t in elapsed_times if t >= min_val)
            else:
                count = sum(1 for t in elapsed_times if min_val <= t < max_val)
            
            percentage = (count / total_count * 100) if total_count > 0 else 0
            
            distribution[bucket_name] = {
                'count': count,
                'percentage': percentage,
                'range': f"{min_val}-{max_val if max_val != float('inf') else 'âˆ'}ms"
            }
        
        return distribution

    def get_gateway_summary(self) -> Dict[str, dict]:
        """è·å–ç½‘å…³æ±‡æ€»ç»Ÿè®¡"""
        summary = {}
        
        for gateway, stats in self.gateway_stats.items():
            if stats['total_requests'] == 0:
                continue
                
            elapsed_times = stats['elapsed_times']
            speeds = stats['speeds']
            file_sizes = stats['file_sizes']
            
            # è®¡ç®—è€—æ—¶åˆ†å¸ƒ
            latency_distribution = self.calculate_latency_distribution(elapsed_times)
            
            summary[gateway] = {
                'total_requests': stats['total_requests'],
                'total_bytes': stats['total_bytes'],
                'avg_elapsed_ms': stats['total_elapsed_ms'] / stats['total_requests'],
                'median_elapsed_ms': statistics.median(elapsed_times),
                'min_elapsed_ms': min(elapsed_times),
                'max_elapsed_ms': max(elapsed_times),
                'std_elapsed_ms': statistics.stdev(elapsed_times) if len(elapsed_times) > 1 else 0,
                'avg_speed_kbps': statistics.mean(speeds),
                'median_speed_kbps': statistics.median(speeds),
                'avg_file_size_bytes': statistics.mean(file_sizes),
                'total_data_mb': stats['total_bytes'] / (1024 * 1024),
                'latency_distribution': latency_distribution
            }
            
        return summary
    
    def calculate_win_rates(self) -> Dict[str, dict]:
        """è®¡ç®—ç½‘å…³win rateï¼ˆæ¯”å…¶ä»–ç½‘å…³å¿«çš„æ¬¡æ•°ï¼‰"""
        print("æ­£åœ¨è®¡ç®—ç½‘å…³win rate...")
        
        # æŒ‰æ–‡ä»¶åˆ†ç»„ï¼ˆä½¿ç”¨mint+subjectä½œä¸ºæ–‡ä»¶æ ‡è¯†ï¼‰
        file_groups = defaultdict(list)
        for entry in self.entries:
            file_key = f"{entry.mint}_{entry.subject}"
            file_groups[file_key].append(entry)
        
        gateway_wins = Counter()
        gateway_participations = Counter()
        comparison_count = 0
        
        for file_key, entries in file_groups.items():
            if len(entries) < 2:  # éœ€è¦è‡³å°‘2ä¸ªç½‘å…³çš„æ•°æ®æ‰èƒ½æ¯”è¾ƒ
                continue
                
            # æ‰¾å‡ºæœ€å¿«çš„ç½‘å…³
            fastest_entry = min(entries, key=lambda x: x.elapsed_ms)
            fastest_gateway = fastest_entry.gateway
            
            # è®°å½•å‚ä¸æ¯”è¾ƒçš„ç½‘å…³
            gateways_in_comparison = set(entry.gateway for entry in entries)
            for gateway in gateways_in_comparison:
                gateway_participations[gateway] += 1
            
            # è®°å½•è·èƒœ
            gateway_wins[fastest_gateway] += 1
            comparison_count += 1
        
        # è®¡ç®—win rate
        win_rates = {}
        for gateway in gateway_participations:
            wins = gateway_wins.get(gateway, 0)
            participations = gateway_participations[gateway]
            win_rate = wins / participations if participations > 0 else 0
            
            win_rates[gateway] = {
                'wins': wins,
                'participations': participations,
                'win_rate': win_rate,
                'win_percentage': win_rate * 100
            }
        
        print(f"åˆ†æäº† {comparison_count} ä¸ªæ–‡ä»¶çš„ç½‘å…³å¯¹æ¯”")
        return win_rates
    
    def simulate_best_gateway_scenario(self) -> dict:
        """æ¨¡æ‹Ÿæ€»æ˜¯ä½¿ç”¨æœ€å¿«ç½‘å…³çš„åœºæ™¯"""
        print("æ­£åœ¨æ¨¡æ‹Ÿæœ€ä¼˜ç½‘å…³é€‰æ‹©åœºæ™¯...")
        
        # æŒ‰æ–‡ä»¶åˆ†ç»„
        file_groups = defaultdict(list)
        for entry in self.entries:
            file_key = f"{entry.mint}_{entry.subject}"
            file_groups[file_key].append(entry)
        
        total_optimal_time = 0
        total_actual_time = 0
        optimal_selections = Counter()
        files_analyzed = 0
        
        for file_key, entries in file_groups.items():
            if len(entries) < 2:
                continue
                
            # æ‰¾å‡ºæœ€å¿«å’Œå®é™…è€—æ—¶
            fastest_entry = min(entries, key=lambda x: x.elapsed_ms)
            actual_total_time = sum(entry.elapsed_ms for entry in entries)
            
            total_optimal_time += fastest_entry.elapsed_ms
            total_actual_time += actual_total_time
            optimal_selections[fastest_entry.gateway] += 1
            files_analyzed += 1
        
        time_saved = total_actual_time - total_optimal_time
        efficiency_gain = (time_saved / total_actual_time * 100) if total_actual_time > 0 else 0
        
        return {
            'files_analyzed': files_analyzed,
            'total_optimal_time_ms': total_optimal_time,
            'total_actual_time_ms': total_actual_time,
            'time_saved_ms': time_saved,
            'efficiency_gain_percentage': efficiency_gain,
            'optimal_gateway_selections': dict(optimal_selections),
            'avg_optimal_time_per_file_ms': total_optimal_time / files_analyzed if files_analyzed > 0 else 0,
            'avg_actual_time_per_file_ms': total_actual_time / files_analyzed if files_analyzed > 0 else 0
        }
    
    def analyze_time_series(self) -> dict:
        """åˆ†ææ—¶åºä¸Šçš„è€—æ—¶å˜åŒ–"""
        print("æ­£åœ¨åˆ†ææ—¶åºå˜åŒ–...")
        
        # æŒ‰æ—¶é—´æ’åº
        sorted_entries = sorted(self.entries, key=lambda x: x.datetime)
        
        if len(sorted_entries) < 2:
            return {}
        
        # æŒ‰å°æ—¶åˆ†ç»„åˆ†æ
        hourly_stats = defaultdict(lambda: defaultdict(list))
        
        for entry in sorted_entries:
            hour_key = entry.datetime.strftime("%Y-%m-%d %H:00")
            hourly_stats[hour_key][entry.gateway].append(entry.elapsed_ms)
        
        # è®¡ç®—æ¯å°æ—¶çš„ç»Ÿè®¡
        time_series = {}
        for hour, gateways in hourly_stats.items():
            hour_stats = {}
            for gateway, times in gateways.items():
                hour_stats[gateway] = {
                    'count': len(times),
                    'avg_ms': statistics.mean(times),
                    'median_ms': statistics.median(times),
                    'min_ms': min(times),
                    'max_ms': max(times)
                }
            time_series[hour] = hour_stats
        
        return time_series
    
    def add_overall_distribution_analysis(self, report: List[str], summary: Dict[str, dict]):
        """æ·»åŠ æ•´ä½“è€—æ—¶åˆ†å¸ƒå¯¹æ¯”åˆ†æ"""
        # å®šä¹‰è€—æ—¶åŒºé—´
        buckets = ['0-100ms', '100-500ms', '500-2000ms', '2000ms+']
        
        # åˆ›å»ºå¯¹æ¯”è¡¨æ ¼
        report.append("å„ç½‘å…³è€—æ—¶åˆ†å¸ƒå¯¹æ¯”:")
        report.append("")
        
        # è¡¨å¤´
        header = f"{'ç½‘å…³':<30} " + " ".join(f"{bucket:>12}" for bucket in buckets)
        report.append(header)
        report.append("-" * len(header))
        
        # æ¯ä¸ªç½‘å…³çš„åˆ†å¸ƒæ•°æ®
        for gateway, stats in summary.items():
            gateway_name = gateway.replace('https://', '').replace('http://', '')
            if len(gateway_name) > 28:
                gateway_name = gateway_name[:25] + "..."
            
            row = f"{gateway_name:<30} "
            latency_dist = stats['latency_distribution']
            
            for bucket in buckets:
                if bucket in latency_dist:
                    count = latency_dist[bucket]['count']
                    percentage = latency_dist[bucket]['percentage']
                    cell = f"{count}({percentage:.0f}%)"
                else:
                    cell = "0(0%)"
                row += f"{cell:>12} "
            
            report.append(row)
        
        report.append("")
        
        # æ·»åŠ åˆ†å¸ƒåˆ†ææ€»ç»“
        report.append("åˆ†å¸ƒåˆ†æ:")
        
        # æ‰¾å‡ºå¿«é€Ÿå“åº”æ¯”ä¾‹æœ€é«˜çš„ç½‘å…³ (0-500ms)
        fast_response_rates = {}
        for gateway, stats in summary.items():
            dist = stats['latency_distribution']
            fast_count = dist.get('0-100ms', {}).get('count', 0) + dist.get('100-500ms', {}).get('count', 0)
            total_count = stats['total_requests']
            fast_rate = (fast_count / total_count * 100) if total_count > 0 else 0
            fast_response_rates[gateway] = fast_rate
        
        if fast_response_rates:
            best_fast_gateway = max(fast_response_rates.items(), key=lambda x: x[1])
            report.append(f"â€¢ å¿«é€Ÿå“åº”ç‡æœ€é«˜ (â‰¤500ms): {best_fast_gateway[0]} ({best_fast_gateway[1]:.1f}%)")
        
        # æ‰¾å‡ºæ…¢å“åº”æ¯”ä¾‹æœ€ä½çš„ç½‘å…³ (â‰¥2000ms)
        slow_response_rates = {}
        for gateway, stats in summary.items():
            dist = stats['latency_distribution']
            slow_count = dist.get('2000ms+', {}).get('count', 0)
            total_count = stats['total_requests']
            slow_rate = (slow_count / total_count * 100) if total_count > 0 else 0
            slow_response_rates[gateway] = slow_rate
        
        if slow_response_rates:
            best_slow_gateway = min(slow_response_rates.items(), key=lambda x: x[1])
            report.append(f"â€¢ æ…¢å“åº”ç‡æœ€ä½ (â‰¥2000ms): {best_slow_gateway[0]} ({best_slow_gateway[1]:.1f}%)")
        
        report.append("")
    
    def generate_report(self) -> str:
        """ç”Ÿæˆç»¼åˆåˆ†ææŠ¥å‘Š"""
        if not self.entries:
            return "æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„æ—¥å¿—æ•°æ®"
        
        self.calculate_stats()
        
        report = []
        report.append("=" * 80)
        report.append("IPFSç½‘å…³æ€§èƒ½åˆ†ææŠ¥å‘Š")
        report.append("=" * 80)
        report.append("")
        
        # åŸºæœ¬ç»Ÿè®¡
        report.append(f"ğŸ“Š åŸºæœ¬ç»Ÿè®¡ä¿¡æ¯")
        report.append(f"   æ€»è¯·æ±‚æ•°: {len(self.entries)}")
        report.append(f"   åˆ†ææ—¶é—´èŒƒå›´: {min(self.entries, key=lambda x: x.datetime).timestamp} åˆ° {max(self.entries, key=lambda x: x.datetime).timestamp}")
        report.append(f"   æ¶‰åŠç½‘å…³æ•°: {len(self.gateway_stats)}")
        report.append("")
        
        # ç½‘å…³æ€§èƒ½æ±‡æ€»
        report.append("ğŸš€ ç½‘å…³æ€§èƒ½æ±‡æ€»")
        report.append("-" * 60)
        summary = self.get_gateway_summary()
        
        for gateway, stats in sorted(summary.items(), key=lambda x: x[1]['avg_elapsed_ms']):
            report.append(f"ç½‘å…³: {gateway}")
            report.append(f"  è¯·æ±‚æ•°: {stats['total_requests']}")
            report.append(f"  å¹³å‡è€—æ—¶: {stats['avg_elapsed_ms']:.1f} ms")
            report.append(f"  ä¸­ä½è€—æ—¶: {stats['median_elapsed_ms']:.1f} ms")
            report.append(f"  è€—æ—¶èŒƒå›´: {stats['min_elapsed_ms']} - {stats['max_elapsed_ms']} ms")
            report.append(f"  æ ‡å‡†å·®: {stats['std_elapsed_ms']:.1f} ms")
            report.append(f"  å¹³å‡é€Ÿåº¦: {stats['avg_speed_kbps']:.1f} KB/s")
            report.append(f"  æ€»æ•°æ®é‡: {stats['total_data_mb']:.2f} MB")
            
            # æ·»åŠ è€—æ—¶åˆ†å¸ƒç»Ÿè®¡
            report.append(f"  è€—æ—¶åˆ†å¸ƒ:")
            latency_dist = stats['latency_distribution']
            for bucket_name, bucket_stats in latency_dist.items():
                count = bucket_stats['count']
                percentage = bucket_stats['percentage']
                if count > 0:
                    report.append(f"    {bucket_name}: {count}æ¬¡ ({percentage:.1f}%)")
            report.append("")
        
        # æ•´ä½“è€—æ—¶åˆ†å¸ƒå¯¹æ¯”
        report.append("ğŸ“Š æ•´ä½“è€—æ—¶åˆ†å¸ƒå¯¹æ¯”")
        report.append("-" * 60)
        self.add_overall_distribution_analysis(report, summary)
        
        # Win Rateåˆ†æ
        report.append("ğŸ† ç½‘å…³Win Rateåˆ†æ")
        report.append("-" * 60)
        win_rates = self.calculate_win_rates()
        
        for gateway, wr_stats in sorted(win_rates.items(), key=lambda x: x[1]['win_rate'], reverse=True):
            report.append(f"ç½‘å…³: {gateway}")
            report.append(f"  è·èƒœæ¬¡æ•°: {wr_stats['wins']} / {wr_stats['participations']}")
            report.append(f"  Win Rate: {wr_stats['win_percentage']:.1f}%")
            report.append("")
        
        # æœ€ä¼˜ç½‘å…³åœºæ™¯åˆ†æ
        report.append("âš¡ æœ€ä¼˜ç½‘å…³é€‰æ‹©åœºæ™¯åˆ†æ")
        report.append("-" * 60)
        optimal_scenario = self.simulate_best_gateway_scenario()
        
        if optimal_scenario:
            report.append(f"åˆ†ææ–‡ä»¶æ•°: {optimal_scenario['files_analyzed']}")
            report.append(f"å½“å‰æ€»è€—æ—¶: {optimal_scenario['total_actual_time_ms']} ms")
            report.append(f"æœ€ä¼˜æ€»è€—æ—¶: {optimal_scenario['total_optimal_time_ms']} ms")
            report.append(f"å¯èŠ‚çœæ—¶é—´: {optimal_scenario['time_saved_ms']} ms")
            report.append(f"æ•ˆç‡æå‡: {optimal_scenario['efficiency_gain_percentage']:.1f}%")
            report.append(f"å¹³å‡å•æ–‡ä»¶è€—æ—¶ (å½“å‰): {optimal_scenario['avg_actual_time_per_file_ms']:.1f} ms")
            report.append(f"å¹³å‡å•æ–‡ä»¶è€—æ—¶ (æœ€ä¼˜): {optimal_scenario['avg_optimal_time_per_file_ms']:.1f} ms")
            report.append("")
            report.append("æœ€ä¼˜ç½‘å…³é€‰æ‹©åˆ†å¸ƒ:")
            for gateway, count in optimal_scenario['optimal_gateway_selections'].items():
                percentage = count / optimal_scenario['files_analyzed'] * 100
                report.append(f"  {gateway}: {count} æ¬¡ ({percentage:.1f}%)")
            report.append("")
        
        # æ—¶åºåˆ†æ
        report.append("ğŸ“ˆ æ—¶åºåˆ†æ (æŒ‰å°æ—¶)")
        report.append("-" * 60)
        time_series = self.analyze_time_series()
        
        if time_series:
            for hour in sorted(time_series.keys())[-10:]:  # æ˜¾ç¤ºæœ€è¿‘10å°æ—¶
                report.append(f"æ—¶é—´: {hour}")
                hour_data = time_series[hour]
                for gateway, stats in hour_data.items():
                    report.append(f"  {gateway}: {stats['count']}æ¬¡, å¹³å‡{stats['avg_ms']:.1f}ms")
                report.append("")
        
        # æ¨èå»ºè®®
        report.append("ğŸ’¡ ä¼˜åŒ–å»ºè®®")
        report.append("-" * 60)
        
        if summary:
            fastest_gateway = min(summary.items(), key=lambda x: x[1]['avg_elapsed_ms'])
            slowest_gateway = max(summary.items(), key=lambda x: x[1]['avg_elapsed_ms'])
            
            report.append(f"1. æ€§èƒ½æœ€ä½³ç½‘å…³: {fastest_gateway[0]} (å¹³å‡ {fastest_gateway[1]['avg_elapsed_ms']:.1f}ms)")
            report.append(f"2. æ€§èƒ½æœ€å·®ç½‘å…³: {slowest_gateway[0]} (å¹³å‡ {slowest_gateway[1]['avg_elapsed_ms']:.1f}ms)")
            
            if optimal_scenario and optimal_scenario['efficiency_gain_percentage'] > 10:
                report.append(f"3. å»ºè®®å®æ–½æ™ºèƒ½ç½‘å…³é€‰æ‹©ï¼Œå¯æå‡æ•ˆç‡ {optimal_scenario['efficiency_gain_percentage']:.1f}%")
            
            # åˆ†æç½‘å…³ç¨³å®šæ€§
            most_stable = min(summary.items(), key=lambda x: x[1]['std_elapsed_ms'])
            report.append(f"4. æœ€ç¨³å®šç½‘å…³: {most_stable[0]} (æ ‡å‡†å·® {most_stable[1]['std_elapsed_ms']:.1f}ms)")
        
        report.append("")
        report.append("=" * 80)
        
        return "\n".join(report)


def main():
    parser = argparse.ArgumentParser(description='IPFSç½‘å…³æ€§èƒ½æ—¥å¿—åˆ†æå™¨')
    parser.add_argument('logfile', nargs='?', help='æ—¥å¿—æ–‡ä»¶è·¯å¾„ (å¦‚æœä¸æä¾›åˆ™ä»stdinè¯»å–)')
    parser.add_argument('--output', '-o', help='è¾“å‡ºæŠ¥å‘Šåˆ°æ–‡ä»¶')
    
    args = parser.parse_args()
    
    analyzer = IPFSLogAnalyzer()
    
    try:
        if args.logfile:
            analyzer.load_log_file(args.logfile)
        else:
            analyzer.load_log_from_stdin()
        
        report = analyzer.generate_report()
        
        if args.output:
            with open(args.output, 'w', encoding='utf-8') as f:
                f.write(report)
            print(f"æŠ¥å‘Šå·²ä¿å­˜åˆ°: {args.output}")
        else:
            print(report)
            
    except KeyboardInterrupt:
        print("\nåˆ†æè¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"é”™è¯¯: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()
